{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import directory_functions as dirfuncs\n",
    "import gather_middleware_statistics as gmws\n",
    "import cut_away_warmup_cooldown as cut\n",
    "from cycler import cycler\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What should be the granularity of the memtier response time interpolation?\n",
    "granularity = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract GET histogram from single memtier logfile\n",
    "def parse_line(line):\n",
    "    split = line.split()\n",
    "    return (split[1], split[2])\n",
    "def extract_GET_histogram(client_logfile_path):\n",
    "    with open(client_logfile_path, \"r\") as logfile:\n",
    "        return [parse_line(line)for line in logfile if line.startswith(\"GET\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_GET_throughput(client_logfile_path):\n",
    "    with open(client_logfile_path, \"r\") as logfile:\n",
    "        for line in logfile:\n",
    "            if line.startswith(\"Gets\"):\n",
    "                split_line = line.split()\n",
    "                return float(split_line[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate_histogram(hist, granularity):\n",
    "    rowcount = hist.shape[0]\n",
    "    desired_steps = np.arange(start=hist[0, 0], stop=hist[rowcount-1, 0], step=granularity)\n",
    "    return desired_steps, np.interp(desired_steps, hist[:,0], hist[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_numrequests(interpolated, xput, granularity):\n",
    "    interpolated = np.vstack(interpolated).transpose()\n",
    "    sorted = interpolated[interpolated[:,0].argsort()]\n",
    "\n",
    "    shifted = np.hstack([sorted[:-1,:], sorted[1:,:]])\n",
    "    shifted_frame = pd.DataFrame(data=shifted, columns=['responsetime_unshifted', 'percentile_unshifted', 'responsetime', 'percentile'])\n",
    "    shifted_frame['num_requests'] = shifted_frame['percentile'] * xput / 100\n",
    "    shifted_frame['num_requests_unshifted'] = shifted_frame['percentile_unshifted'] * xput / 100\n",
    "    shifted_frame['requests'] = (shifted_frame['num_requests'] - shifted_frame['num_requests_unshifted'])\n",
    "\n",
    "    return shifted_frame.loc[:, ['responsetime', 'percentile', 'requests']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_multiple_histograms(histogram_arrays, throughputs, granularity):\n",
    "    interpolated = [interpolate_histogram(hist, granularity) for hist in histogram_arrays]\n",
    "    interpolated_with_numrequests = [calculate_numrequests(interp, xput, granularity) for interp, xput in zip(interpolated, throughputs)]\n",
    "    return pd.concat(interpolated_with_numrequests).groupby('responsetime').agg({'requests': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting histograms for Sharded\n",
    "exp5_1_dir = \"\" # 5.1 sharded experiment dir\n",
    "experiments = [\"sharded_1multiget\", \"sharded_3multiget\", \"sharded_6multiget\", \"sharded_9multiget\"]\n",
    "client_logfiles = [\"client_01_0.log\", \"client_01_1.log\", \"client_02_0.log\", \"client_02_1.log\", \"client_03_0.log\", \"client_03_1.log\"]\n",
    "reps = 3\n",
    "exp_filepaths = [[os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles] for experiment in experiments]\n",
    "exp_histograms = [[np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths] for filepaths in exp_filepaths]\n",
    "exp_throughputs = [[extract_GET_throughput(filepath) for filepath in filepaths] for filepaths in exp_filepaths]\n",
    "hist_data_sharded = [combine_multiple_histograms(histograms, throughputs, granularity) for histograms, throughputs in zip(exp_histograms, exp_throughputs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting histograms for Non-Sharded\n",
    "exp5_2_dir = \"\" # 5.2 nonsharded experiment dir\n",
    "experiments = [\"nonsharded_1multiget\", \"nonsharded_3multiget\", \"nonsharded_6multiget\", \"nonsharded_9multiget\"]\n",
    "client_logfiles = [\"client_01_0.log\", \"client_01_1.log\", \"client_02_0.log\", \"client_02_1.log\", \"client_03_0.log\", \"client_03_1.log\"]\n",
    "reps = 3\n",
    "exp_filepaths = [[os.path.join(exp5_2_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles] for experiment in experiments]\n",
    "exp_histograms = [[np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths] for filepaths in exp_filepaths]\n",
    "exp_throughputs = [[extract_GET_throughput(filepath) for filepath in filepaths] for filepaths in exp_filepaths]\n",
    "hist_data_nonsharded = [combine_multiple_histograms(histograms, throughputs, granularity) for histograms, throughputs in zip(exp_histograms, exp_throughputs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total histograms\n",
    "xrng = [0, 4]\n",
    "buckets = 20\n",
    "num_subplots = len(hist_data_sharded)\n",
    "fig, axes = plt.subplots(num_subplots, 2, sharey=True, sharex=True, figsize=(10,10))\n",
    "for i in range(0, num_subplots):\n",
    "    axes[i, 0].hist(hist_data_sharded[i]['responsetime'], buckets, weights=hist_data_sharded[i]['requests'], range=xrng, normed=1, facecolor='green', alpha=0.75)\n",
    "    axes[i, 1].hist(hist_data_nonsharded[i]['responsetime'], buckets, weights=hist_data_nonsharded[i]['requests'], range=xrng, normed=1, facecolor='blue', alpha=0.75)\n",
    "    \n",
    "#axes.set_xlim(range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Percentile plot\n",
    "multigets = [1, 3, 6, 9]\n",
    "desired_percentiles = [.25, .50, .75, .90, .99]\n",
    "\n",
    "# Taken from http://yiqun-dai.blogspot.ch/2017/03/weighted-percentile-in-python-pandas.html\n",
    "def wquantile(x,q):           \n",
    "    xsort = x.sort_values(x.columns[0])\n",
    "    xsort['index'] = range(len(x))\n",
    "    p = q * x[x.columns[1]].sum()\n",
    "    pop = float(xsort[xsort.columns[1]][xsort['index']==0])\n",
    "    i = 0\n",
    "    while pop < p:\n",
    "        pop = pop + float(xsort[xsort.columns[1]][xsort['index']==i+1])\n",
    "        i = i + 1\n",
    "    return xsort[xsort.columns[0]][xsort['index']==i]\n",
    "\n",
    "def extract_percentiles(hist_data, multigets, percentiles):\n",
    "    percentile_list = []\n",
    "    for i in range(0,4):\n",
    "        data = hist_data[i]\n",
    "        percentile_list.extend([(multigets[i], perc, wquantile(data, perc)) for perc in percentiles])\n",
    "    return pd.DataFrame(data=percentile_list, columns=['multigets', 'percentile', 'responsetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sharded_plot_percentiles = extract_percentiles(hist_data_sharded, multigets, desired_percentiles)\n",
    "nonsharded_plot_percentiles = extract_percentiles(hist_data_nonsharded, multigets, desired_percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "width=0.1\n",
    "gap = 0.6\n",
    "bargap=0.005\n",
    "x = np.concatenate([np.arange(start=1+(i*gap), stop=1+(i*gap)+4*(width+bargap), step=(width+bargap)) for i in range(0, 5)])\n",
    "#for i in range(0,5):\n",
    "#    print(x[(4*i):(4*i)+4])\n",
    "def plot_percentiles_bar(ax, data):\n",
    "    color_cycler = cycler('color', ['#fdbb84', '#fc8d59', '#ef6548', '#d7301f', '#7f0000'])\n",
    "    ax.set_prop_cycle(color_cycler)\n",
    "    for i, (key, grp) in enumerate(data.groupby(['percentile'])):\n",
    "        ax.bar(x[(4*i):(4*i)+4], grp['responsetime'], width=width)\n",
    "        #ax.set_ylim([0,10])\n",
    "        #ax.set_xlim([0.5,4.5])\n",
    "        ax.set_xticks(x)\n",
    "        #ax.set_xticklabels(grp['multigets'])\n",
    "        ax.set_xticklabels(('1', '3', '6', '9', '1', '3', '6', '9', '1', '3', '6', '9', '1', '3', '6', '9', '1', '3', '6', '9'))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, sharex=False, sharey=True, figsize=(10, 6))\n",
    "\n",
    "plot_percentiles_bar(axes[0], sharded_plot_percentiles)\n",
    "plot_percentiles_bar(axes[1], nonsharded_plot_percentiles)\n",
    "\n",
    "fig.text(0.5, 0.02, 'Number of keys in GETs', ha='center')\n",
    "fig.text(0.07, 0.5, 'Response Time (ms)', va='center', rotation='vertical')\n",
    "fig.text(0.085, 0.7, 'Sharded', va='center', rotation='vertical')\n",
    "fig.text(0.085, 0.3, 'Non-sharded', va='center', rotation='vertical')\n",
    "fig.text(0.21, 0.06, '25th perc.', ha='center')\n",
    "fig.text(0.36, 0.06, '50th perc.', ha='center')\n",
    "fig.text(0.51, 0.06, '75th perc.', ha='center')\n",
    "fig.text(0.66, 0.06, '90th perc.', ha='center')\n",
    "fig.text(0.81, 0.06, '99th perc.', ha='center')\n",
    "fig.text(0.21, 0.477, '25th perc.', ha='center')\n",
    "fig.text(0.36, 0.477, '50th perc.', ha='center')\n",
    "fig.text(0.51, 0.477, '75th perc.', ha='center')\n",
    "fig.text(0.66, 0.477, '90th perc.', ha='center')\n",
    "fig.text(0.81, 0.477, '99th perc.', ha='center')\n",
    "plt.show()\n",
    "fig.savefig(\"./graphs/exp5/responsetime_percentiles.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get response times from MW for 6 multigets\n",
    "def gather_requests(exp_dir, reps, middlewares):\n",
    "    all_reps = []\n",
    "    for rep in reps:\n",
    "\n",
    "        middleware_dirs = [dirfuncs.get_only_subdir(os.path.join(exp_dir, str(rep), mw_dir)) for mw_dir in middlewares]\n",
    "        concatenated_requests = [gmws.concatenate_requestlogs(middleware_dir) for middleware_dir in middleware_dirs]\n",
    "\n",
    "\n",
    "        filtered_requests = [reqs[reqs['requestType'].str.contains(\"GET\")] for reqs in concatenated_requests]\n",
    "        metrics = [gmws.extract_metrics_old(reqs) for reqs in filtered_requests]\n",
    "\n",
    "\n",
    "        cut_metrics = [cut.cut_away_warmup_cooldown(mets, 10, 72) for mets in metrics]\n",
    "\n",
    "        all_reps.extend(cut_metrics)\n",
    "        \n",
    "    return pd.concat(all_reps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5.3: Data gathering\n",
    "# Extracting histograms for Sharded\n",
    "exp_dir_shard_rerun = \"\" # 5.1 sharded small-rerun experiment dir\n",
    "experiments = [\"sharded_6multiget\"]\n",
    "client_logfiles = [\"client_01_0.log\", \"client_01_1.log\", \"client_02_0.log\", \"client_02_1.log\", \"client_03_0.log\", \"client_03_1.log\"]\n",
    "#client_logfiles = [\"client_03_0.log\", \"client_02_0.log\", \"client_03_0.log\"]\n",
    "#client_logfiles = [\"client_01_1.log\", \"client_02_1.log\", \"client_03_1.log\"]\n",
    "reps = 3\n",
    "exp_filepaths = [[os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles] for experiment in experiments]\n",
    "exp_histograms = [[np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths] for filepaths in exp_filepaths]\n",
    "exp_throughputs = [[extract_GET_throughput(filepath) for filepath in filepaths] for filepaths in exp_filepaths]\n",
    "hist_data_sharded_rerun = [combine_multiple_histograms(histograms, throughputs, granularity) for histograms, throughputs in zip(exp_histograms, exp_throughputs)]\n",
    "\n",
    "\n",
    "exp_dir_shard = os.path.join(exp5_1_dir, \"sharded_6multiget\")\n",
    "exp_dir_nonshard = os.path.join(exp5_2_dir, \"nonsharded_6multiget\")\n",
    "reps = range(1,4)\n",
    "middlewares = [\"middleware_04\", \"middleware_05\"]\n",
    "#middlewares = [\"middleware_04\"]\n",
    "#middlewares = [\"middleware_05\"]\n",
    "\n",
    "sharded_reqs_mw = gather_requests(exp_dir_shard, reps, middlewares)\n",
    "nonsharded_reqs_mw = gather_requests(exp_dir_nonshard, reps, middlewares)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Section 5.3: Plot total histograms\n",
    "\n",
    "sharded_reqs_mt = hist_data_sharded_rerun[0]\n",
    "nonsharded_reqs_mt = hist_data_nonsharded[2]\n",
    "xrng = [0, 4]\n",
    "buckets = 40\n",
    "num_subplots = len(hist_data_sharded)\n",
    "fig, axes = plt.subplots(2, 2, sharey=True, sharex=True, figsize=(6,4))\n",
    "\n",
    "axes[0,0].hist(sharded_reqs_mt['responsetime'], buckets, weights=sharded_reqs_mt['requests'], range=xrng, normed=1, facecolor='#33a02c', alpha=0.75)\n",
    "axes[0,1].hist(nonsharded_reqs_mt['responsetime'], buckets, weights=nonsharded_reqs_mt['requests'], range=xrng, normed=1, facecolor='#33a02c', alpha=0.75)\n",
    "axes[1,0].hist(sharded_reqs_mw['responseTime_ms'], buckets, range=xrng, normed=1, facecolor='#1f78b4', alpha=0.75)\n",
    "axes[1,1].hist(nonsharded_reqs_mw['responseTime_ms'], buckets, range=xrng, normed=1, facecolor='#1f78b4', alpha=0.75)\n",
    "\n",
    "axes[0,0].set_ylabel(\"Normed frequency\")\n",
    "axes[1,0].set_ylabel(\"Normed frequency\")\n",
    "axes[1,0].set_xlabel(\"Response Time (ms)\")\n",
    "axes[1,1].set_xlabel(\"Response Time (ms)\")\n",
    "\n",
    "axes[0,0].set_title(\"Sharded (Memtier)\")\n",
    "axes[0,1].set_title(\"Non-Sharded (Memtier)\")\n",
    "axes[1,0].set_title(\"Sharded (Middleware)\")\n",
    "axes[1,1].set_title(\"Non-Sharded (Middleware)\")\n",
    "\n",
    "#axes[0,0]\n",
    "#axes[0,1]\n",
    "#axes[1,0]\n",
    "#axes[1,1]\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"./graphs/exp5/5_3_histogram.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting response times on different middlewares\n",
    "reps = range(1,4)\n",
    "\n",
    "sharded_mw1 = gather_requests(exp_dir_shard, reps, [\"middleware_04\"])\n",
    "sharded_mw2 = gather_requests(exp_dir_shard, reps, [\"middleware_05\"])\n",
    "\n",
    "nonsharded_mw1 = gather_requests(exp_dir_nonshard, reps, [\"middleware_04\"])\n",
    "nonsharded_mw2 = gather_requests(exp_dir_nonshard, reps, [\"middleware_05\"])\n",
    "\n",
    "client_logfiles_mw1 = [\"client_01_0.log\", \"client_02_0.log\", \"client_03_0.log\"]\n",
    "client_logfiles_mw2 = [\"client_01_1.log\", \"client_02_1.log\", \"client_03_1.log\"]\n",
    "experiment = \"sharded_6multiget\"\n",
    "reps = 3\n",
    "\n",
    "filepaths_mw1 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles_mw1]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw1]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw1]\n",
    "histo_sharded_frommt_mw1 = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "\n",
    "filepaths_mw2 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles_mw2]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw2]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw2]\n",
    "histo_sharded_frommt_mw2 = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "experiment = \"nonsharded_6multiget\"\n",
    "reps = 3\n",
    "\n",
    "filepaths_mw1 = [os.path.join(exp5_2_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles_mw1]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw1]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw1]\n",
    "histo_nonsharded_frommt_mw1 = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "\n",
    "filepaths_mw2 = [os.path.join(exp5_2_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in client_logfiles_mw2]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw2]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw2]\n",
    "histo_nonsharded_frommt_mw2 = combine_multiple_histograms(histograms, throughputs, granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Plotting response times on different middlewares\n",
    "xrng = [0, 4]\n",
    "buckets = 40\n",
    "num_subplots = len(hist_data_sharded)\n",
    "fig, axes = plt.subplots(1,2, sharey=True, figsize=(6,2))\n",
    "\n",
    "#axes[0].hist(histo_sharded_frommt_mw1['responsetime'], buckets, weights=histo_sharded_frommt_mw1['requests'], range=xrng, normed=1, facecolor='green', alpha=0.5)\n",
    "#axes[0].hist(histo_sharded_frommt_mw2['responsetime'], buckets, weights=histo_sharded_frommt_mw2['requests'], range=xrng, normed=1, facecolor='green', alpha=0.5)\n",
    "\n",
    "#axes[1].hist(histo_nonsharded_frommt_mw1['responsetime'], buckets, weights=histo_nonsharded_frommt_mw1['requests'], range=xrng, normed=1, facecolor='green', alpha=0.5)\n",
    "#axes[1].hist(histo_nonsharded_frommt_mw2['responsetime'], buckets, weights=histo_nonsharded_frommt_mw2['requests'], range=xrng, normed=1, facecolor='green', alpha=0.5)\n",
    "\n",
    "axes[0].hist(sharded_mw1['responseTime_ms'], buckets, range=xrng, normed=1, facecolor='#984ea3', alpha=0.5)\n",
    "axes[0].hist(sharded_mw2['responseTime_ms'], buckets, range=xrng, normed=1, facecolor='#4daf4a', alpha=0.5)\n",
    "\n",
    "axes[1].hist(nonsharded_mw1['responseTime_ms'], buckets, range=xrng, normed=1, facecolor='#984ea3', alpha=0.5)\n",
    "axes[1].hist(nonsharded_mw2['responseTime_ms'], buckets, range=xrng, normed=1, label=\"asd\", facecolor='#4daf4a', alpha=0.5)\n",
    "\n",
    "axes[0].set_xlabel(\"Response Time (ms)\")\n",
    "axes[1].set_xlabel(\"Response Time (ms)\")\n",
    "\n",
    "axes[0].set_ylabel(\"Normed frequency\")\n",
    "\n",
    "axes[0].set_title(\"Sharded middlewares\")\n",
    "axes[1].set_title(\"Non-sharded middlewares\")\n",
    "\n",
    "axes[0].legend(('MW1', 'MW2'),\n",
    "              loc='upper right')\n",
    "plt.show()\n",
    "fig.savefig(\"./graphs/exp5/5_3_histogram_singlemiddlewares.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = os.path.join(\"/home/flo/Documents/eth-asl-final-experiment-data/exp5/5_1_multigets_sharded_2017-11-24_085912/sharded_6multiget/1\", \"client_02_0.log\")\n",
    "hist = extract_GET_histogram(filepath)\n",
    "xput = extract_GET_throughput(filepath)\n",
    "hist_array = np.array(hist, dtype=float)\n",
    "interpolated = interpolate_histogram(hist_array, granularity)\n",
    "with_numrequests = calculate_numrequests(np.vstack(interpolated).transpose(), xput, granularity)\n",
    "with_numrequests\n",
    "\n",
    "xrng = [0, 4]\n",
    "buckets = 40\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(with_numrequests['responsetime'], buckets, weights=with_numrequests['requests'], range=xrng, normed=1, facecolor='green', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EXP 5.1 Sharded. Why is Resp time for 6multiget slower than 9? Why is 9 Multiget faster than 6?\n",
    "dirpaths = [os.path.join(exp5_1_dir, \"sharded_{}multiget\".format(multiget)) for multiget in [1,3,6,9]]\n",
    "reps = range(1,4)\n",
    "middlewares = [\"middleware_04\", \"middleware_05\"]\n",
    "\n",
    "reqs_combined = [gather_requests(dir, reps, middlewares) for dir in dirpaths]\n",
    "reqs_mw1 = [gather_requests(dir, reps, [\"middleware_04\"]) for dir in dirpaths]\n",
    "reqs_mw2 = [gather_requests(dir, reps, [\"middleware_05\"]) for dir in dirpaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## EXP 5.1 Sharded. Why is Resp time for 6multiget slower than 9? Why is 9 Multiget faster than 6?\n",
    "xrng = [0, 4]\n",
    "buckets = 100\n",
    "multigets = [1,3,6,9]\n",
    "metricname = 'memcachedRTT_ms'\n",
    "fig, axes = plt.subplots(4,3, sharey=True, sharex=True, figsize=(8,6))\n",
    "axes[0,0].set_xlim([0,2])\n",
    "for i in range(0,4):\n",
    "    axes[i,0].hist(reqs_combined[i][metricname], buckets, range=xrng, normed=1, facecolor='#33a02c')\n",
    "    axes[i,1].hist(reqs_mw1[i][metricname], buckets, range=xrng, normed=1, facecolor='#b2df8a')\n",
    "    axes[i,2].hist(reqs_mw2[i][metricname], buckets, range=xrng, normed=1, facecolor='#a6cee3')\n",
    "    \n",
    "axes[0,0].set_title(\"Both middlewares combined\")\n",
    "axes[0,1].set_title(\"Middleware 1\")\n",
    "axes[0,2].set_title(\"Middleware 2\")\n",
    "axes[3,0].set_xlabel(\"Response Time (ms)\")\n",
    "axes[3,1].set_xlabel(\"Response Time (ms)\")\n",
    "axes[3,2].set_xlabel(\"Response Time (ms)\")\n",
    "\n",
    "axes[0,0].set_ylabel(\"Single-key GETs\")\n",
    "axes[1,0].set_ylabel(\"3-key MultiGETs\")\n",
    "axes[2,0].set_ylabel(\"6-key MultiGETs\")\n",
    "axes[3,0].set_ylabel(\"9-key MultiGETs\")\n",
    "\n",
    "plt.xticks([0,0.5,1,1.5,2])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"./graphs/exp5/memcachedRTT_histograms.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EXP 5.1 Sharded. Why is Resp time for 6multiget slower than 9? Why is 9 Multiget faster than 6?\n",
    "dirpaths = [os.path.join(exp5_1_dir, \"sharded_{}multiget\".format(multiget)) for multiget in [1,3,6,9]]\n",
    "reps = [1]\n",
    "middlewares = [\"middleware_04\", \"middleware_05\"]\n",
    "\n",
    "\n",
    "xrng = [0, 4]\n",
    "buckets = 100\n",
    "columnname = 'memcachedRTT_ms'\n",
    "fig, axes = plt.subplots(1,3, sharey=True, sharex=True, figsize=(10,2))\n",
    "axes[0].set_xlim([0,4])\n",
    "for rep in range(1,4):\n",
    "    reqs_combined = [gather_requests(dir, [rep], middlewares) for dir in dirpaths]\n",
    "    reqs_mw1 = [gather_requests(dir, [rep], [\"middleware_04\"]) for dir in dirpaths]\n",
    "    reqs_mw2 = [gather_requests(dir, [rep], [\"middleware_05\"]) for dir in dirpaths]\n",
    "    \n",
    "    \n",
    "    i=2\n",
    "    axes[0].hist(reqs_combined[i][columnname], buckets, range=xrng, normed=1, facecolor='#33a02c', alpha=0.33)\n",
    "    axes[1].hist(reqs_mw1[i][columnname], buckets, range=xrng, normed=1, facecolor='#b2df8a', alpha=0.33)\n",
    "    axes[2].hist(reqs_mw2[i][columnname], buckets, range=xrng, normed=1, facecolor='#a6cee3', alpha=0.33)\n",
    "\n",
    "axes[0].set_title(\"Both middlewares combined\")\n",
    "axes[1].set_title(\"Middleware 1\")\n",
    "axes[2].set_title(\"Middleware 2\")\n",
    "axes[0].set_xlabel(\"Response Time (ms)\")\n",
    "axes[1].set_xlabel(\"Response Time (ms)\")\n",
    "axes[2].set_xlabel(\"Response Time (ms)\")\n",
    "axes[0].set_ylabel(\"6-key MultiGETs\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(\"./graphs/exp5/\", \"sharded_6multiget_mw_histogram.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## EXP 5.1 Re-run Sharded to check 6-multiget anomaly\n",
    "dirpaths = [os.path.join(\"/home/flo/Documents/eth-asl-final-experiment-data/exp5/5_1_sharded_small_re-run_2017-12-05_193553\", \"sharded_{}multiget\".format(multiget)) for multiget in [6,9]]\n",
    "reps = range(1,4)\n",
    "middlewares = [\"middleware_04\", \"middleware_05\"]\n",
    "\n",
    "reqs_combined = [gather_requests(dir, reps, middlewares) for dir in dirpaths]\n",
    "reqs_mw1 = [gather_requests(dir, reps, [\"middleware_04\"]) for dir in dirpaths]\n",
    "reqs_mw2 = [gather_requests(dir, reps, [\"middleware_05\"]) for dir in dirpaths]\n",
    "xrng = [0, 4]\n",
    "buckets = 100\n",
    "multigets = [6,9]\n",
    "metricname = 'memcachedRTT_ms'\n",
    "fig, axes = plt.subplots(2,3, sharey=True, sharex=True, figsize=(10,4))\n",
    "axes[0,0].set_xlim([0,5])\n",
    "for i in range(0,2):\n",
    "    axes[i,0].hist(reqs_combined[i][metricname], buckets, range=xrng, normed=1, facecolor='#33a02c')\n",
    "    axes[i,1].hist(reqs_mw1[i][metricname], buckets, range=xrng, normed=1, facecolor='#b2df8a')\n",
    "    axes[i,2].hist(reqs_mw2[i][metricname], buckets, range=xrng, normed=1, facecolor='#a6cee3')\n",
    "\n",
    "axes[0,0].set_title(\"Both middlewares combined\")\n",
    "axes[0,1].set_title(\"Middleware 1\")\n",
    "axes[0,2].set_title(\"Middleware 2\")\n",
    "axes[1,0].set_xlabel(\"Response Time (ms)\")\n",
    "axes[1,1].set_xlabel(\"Response Time (ms)\")\n",
    "axes[1,2].set_xlabel(\"Response Time (ms)\")\n",
    "\n",
    "axes[0,0].set_ylabel(\"6-key MultiGETs\")\n",
    "axes[1,0].set_ylabel(\"9-key MultiGETs\")\n",
    "plt.xticks([0,1.0,2.0,3.0,4.0,5.0])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"./graphs/exp5/memcachedRTT_histograms_rerun.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Plotting response times for different clients\n",
    "reps = range(1,4)\n",
    "exp5_1_dir = \"\" # 5.1 sharded experiment dir\n",
    "logfile1 = [\"client_01_0.log\"]\n",
    "logfile2 = [\"client_02_0.log\"]\n",
    "logfile3 = [\"client_03_0.log\"]\n",
    "logfile4 = [\"client_01_1.log\"]\n",
    "logfile5 = [\"client_02_1.log\"]\n",
    "logfile6 = [\"client_03_1.log\"]\n",
    "experiment = \"sharded_6multiget\"\n",
    "reps = 3\n",
    "\n",
    "filepaths_mw1 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in logfile1]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw1]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw1]\n",
    "client1_hist = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "\n",
    "filepaths_mw2 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in logfile2]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw2]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw2]\n",
    "client2_hist = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "filepaths_mw2 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in logfile3]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw2]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw2]\n",
    "client3_hist = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "filepaths_mw1 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in logfile4]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw1]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw1]\n",
    "client4_hist = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "\n",
    "filepaths_mw2 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in logfile5]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw2]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw2]\n",
    "client5_hist = combine_multiple_histograms(histograms, throughputs, granularity)\n",
    "\n",
    "filepaths_mw2 = [os.path.join(exp5_1_dir, experiment, str(rep), logfiles) for rep in range(1, reps+1) for logfiles in logfile6]\n",
    "histograms = [np.array(extract_GET_histogram(filepath), dtype=float) for filepath in filepaths_mw2]\n",
    "throughputs = [extract_GET_throughput(filepath) for filepath in filepaths_mw2]\n",
    "client6_hist = combine_multiple_histograms(histograms, throughputs, granularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, sharey=True, figsize=(6,2))\n",
    "xrng = [0, 4]\n",
    "buckets = 40\n",
    "axes[0].hist(client1_hist['responsetime'], buckets, weights=client1_hist['requests'], range=xrng, normed=1, facecolor='#fec44f', alpha=0.33)\n",
    "axes[0].hist(client2_hist['responsetime'], buckets, weights=client2_hist['requests'], range=xrng, normed=1, facecolor='#4daf4a', alpha=0.33)\n",
    "axes[0].hist(client3_hist['responsetime'], buckets, weights=client3_hist['requests'], range=xrng, normed=1, facecolor='#984ea3', alpha=0.33)\n",
    "\n",
    "axes[1].hist(client4_hist['responsetime'], buckets, weights=client4_hist['requests'], range=xrng, normed=1, facecolor='#fec44f', alpha=0.33)\n",
    "axes[1].hist(client5_hist['responsetime'], buckets, weights=client5_hist['requests'], range=xrng, normed=1, facecolor='#4daf4a', alpha=0.33)\n",
    "axes[1].hist(client6_hist['responsetime'], buckets, weights=client6_hist['requests'], range=xrng, normed=1, facecolor='#984ea3', alpha=0.33)\n",
    "\n",
    "axes[0].set_xlabel(\"Response Time (ms)\")\n",
    "axes[1].set_xlabel(\"Response Time (ms)\")\n",
    "\n",
    "axes[0].set_ylabel(\"Normed frequency\")\n",
    "\n",
    "axes[0].set_title(\"Clients to middleware 1\")\n",
    "axes[1].set_title(\"Clients to middleware 2\")\n",
    "\n",
    "axes[1].legend(('Client 1', 'Client 2', 'Client 3'),\n",
    "              loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "fig.savefig(\"./graphs/exp5/5_3_histogram_sharded_singleclients.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
