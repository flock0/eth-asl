{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gather_memtier_statistics as gmts\n",
    "import cut_away_warmup_cooldown as cut\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp6_dir = \"\" # 6 2kanalysis experiment dir\n",
    "outdir = \"./graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_single_xputresp(experiment_dir, rep, client_logfiles):\n",
    "    client_logfile_paths = [os.path.join(experiment_dir, str(rep), client_logfile) for client_logfile in client_logfiles]\n",
    "    client_metrics = gmts.aggregate_over_clients(client_logfile_paths)\n",
    "\n",
    "    cut_client_metrics = cut.cut_away_warmup_cooldown(client_metrics, warmup_end, cooldown_start)\n",
    "\n",
    "    return gmts.aggregate_over_timesteps(cut_client_metrics).loc['mean', ['throughput', 'responsetime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_all_rawdata(inputdir, workload, mw_settings, mc_settings, worker_settings, reps, client_logfiles, warmup_end, cooldown_start):\n",
    "    result_list = []\n",
    "    for num_mws in mw_settings:\n",
    "        for num_mcs in mc_settings:\n",
    "            for num_workers in worker_settings:\n",
    "                experiment_dir = os.path.join(inputdir, \"{}_{}mw{}mc{}workers\".format(workload, num_mws, num_mcs, num_workers))\n",
    "                for rep in range(1, reps+1):\n",
    "                    xput, resp = gather_single_xputresp(experiment_dir, rep, client_logfiles)\n",
    "                    result_list.append((num_mws, num_mcs, num_workers, rep, xput, resp))\n",
    "                    \n",
    "    df = pd.DataFrame(data=result_list, columns=['MW', 'MC', 'WT', 'rep', 'xput', 'resp'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data gathering\n",
    "inputdir = exp6_dir\n",
    "workload = \"fiftyFifty\"\n",
    "metric_name = 'resp'\n",
    "mw_settings = [1, 2]\n",
    "mc_settings = [2, 3]\n",
    "worker_settings = [8, 32]\n",
    "reps = 3\n",
    "client_logfiles = client_logfiles = [\"client_01_0.log\", \"client_01_1.log\", \"client_02_0.log\", \"client_02_1.log\", \"client_03_0.log\", \"client_03_1.log\"]\n",
    "warmup_end = 10\n",
    "cooldown_start = 72\n",
    "should_logtransform=True\n",
    "\n",
    "raw = gather_all_rawdata(inputdir, workload, mw_settings, mc_settings, worker_settings, reps, client_logfiles, warmup_end, cooldown_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiplicative Model\n",
    "# Check ratio of y_max/y_max\n",
    "ys = raw[metric_name]\n",
    "print (\"Max/Min Ratio\", ys.max()/ys.min())\n",
    "\n",
    "if should_logtransform:\n",
    "    raw[metric_name] = np.log(raw[metric_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averages = [(key[0], key[1], key[2], grp[metric_name].mean()) for key, grp in raw.groupby(['MW', 'MC', 'WT'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_table = pd.DataFrame(averages, columns=['MW', 'MC', 'WT', 'y_mean'])\n",
    "analysis_table.insert(0, 'I', 1)\n",
    "\n",
    "analysis_table['MW'].replace([1, 2], [-1, 1], inplace=True)\n",
    "analysis_table['MC'].replace([2, 3], [-1, 1], inplace=True)\n",
    "analysis_table['WT'].replace([8, 32], [-1, 1], inplace=True)\n",
    "\n",
    "analysis_table.insert(4, 'MWMC', analysis_table['MW'] * analysis_table['MC'])\n",
    "analysis_table.insert(5, 'MWWT', analysis_table['MW'] * analysis_table['WT'])\n",
    "analysis_table.insert(6, 'MCWT', analysis_table['MC'] * analysis_table['WT'])\n",
    "analysis_table.insert(7, 'MWMCWT', analysis_table['MW'] * analysis_table['MC'] * analysis_table['WT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "effects_names = ['I', 'MW', 'MC', 'WT', 'MWMC', 'MWWT', 'MCWT', 'MWMCWT']\n",
    "a = analysis_table[effects_names]\n",
    "b = analysis_table['y_mean']\n",
    "solution = np.linalg.solve(np.array(a), np.array(b))\n",
    "effects = pd.DataFrame(data=solution, index=effects_names, columns=['Effects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Allocation of Variation\n",
    "SSY = (raw[metric_name]**2).sum()\n",
    "SS_effects = 2**3*3*(solution**2)\n",
    "SST = SSY - SS_effects[0]\n",
    "SSE = SSY - 2**3*3*((solution**2).sum())\n",
    "\n",
    "SS_with_error = np.append(SS_effects[1:], SSE)\n",
    "pd.DataFrame(SS_with_error/SST*100, index=effects_names[1:] + [\"Error\"], columns=[\"Explanatory Effect (%)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Confidence Intervals\n",
    "confidence_alpha = 0.1 # Take 90 % conf intervals\n",
    "degrees_of_freedom = (2**3*(reps-1))# From the book\n",
    "stddev_err = np.sqrt(SSE/degrees_of_freedom)\n",
    "stddev_effect = stddev_err/np.sqrt(2**3*3)\n",
    "t_val = stats.t.ppf(1-(confidence_alpha/2),degrees_of_freedom)\n",
    "confidence_interval_oneside = stddev_effect * t_val\n",
    "\n",
    "confidence_lowerbound = effects - confidence_interval_oneside\n",
    "confidence_upperbound = effects + confidence_interval_oneside\n",
    "effects_wConfIntervals = pd.concat([effects, confidence_lowerbound, confidence_upperbound], axis=1)\n",
    "effects_wConfIntervals.columns = ['Effect', 'Lower Confidence Bound', 'Upper Confidence Bound']\n",
    "effects_wConfIntervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check regression assumptions: \n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "## QQ-plot\n",
    "avg = pd.DataFrame(data=averages, columns=['MW', 'MC', 'WT', metric_name])\n",
    "merged = pd.merge(raw, avg, how='left', on=['MW', 'MC', 'WT'])\n",
    "errors = merged[metric_name + '_x'] - merged[metric_name + '_y']\n",
    "\n",
    "stats.probplot(errors, dist=\"norm\", plot=axes[0])\n",
    "\n",
    "## Pred vs. Resid\n",
    "predicted = np.dot(a,solution).repeat(reps)\n",
    "residuals = raw[metric_name] - predicted\n",
    "axes[1].scatter(predicted, residuals, color='b', marker='o', alpha=1)\n",
    "axes[1].axhline(y=0, color='r')\n",
    "axes[1].set_xlabel(\"Predicted throughput\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].set_title(\"Error distribution\")\n",
    "plt.show()\n",
    "fig.savefig(os.path.join(outdir, \"{}_{}_modelassumptions.png\".format(workload, metric_name)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
